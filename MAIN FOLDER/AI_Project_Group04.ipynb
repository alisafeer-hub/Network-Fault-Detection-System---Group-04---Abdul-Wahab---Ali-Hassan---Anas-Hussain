{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27381235-6801-4901-b160-bd606cbf5c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset of 2000 rows created successfully!\n",
      "File saved as: sensor_fault_dataset_v2.csv\n",
      "Total faults injected: 1500 cells (15%)\n",
      "\n",
      "First 5 rows of the generated DataFrame:\n",
      "            Timestamp          RPM     GPS_Speed       Accel_X   Accel_Y  \\\n",
      "0 2024-01-01 12:06:25  2974.135577  4.321762e+01  3.736169e+10  2.758492   \n",
      "1 2024-01-01 13:36:24  3101.897721  4.164591e+00 -3.677078e-02  3.087882   \n",
      "2 2024-01-01 13:38:35  3177.797840  4.742248e+01 -1.691346e-01  2.110747   \n",
      "3 2024-01-01 16:58:07  3076.118068  3.911582e+01 -2.819793e-03  2.122932   \n",
      "4 2024-01-01 17:52:03  3103.368652  4.344176e+08 -1.252964e-02  2.539210   \n",
      "\n",
      "    Accel_Z  \n",
      "0  9.838083  \n",
      "1  9.934272  \n",
      "2  9.826516  \n",
      "3  9.735100  \n",
      "4  9.888361  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_ROWS = 2000\n",
    "FAULT_RATE = 0.15 # 15% of all data points will be faulty\n",
    "SEED = 42 # for reproducibility\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- 1. Generate Fully Randomized Historic Timestamps (Before 2025) ---\n",
    "\n",
    "# Define the historical range: 1 year (Jan 1, 2024 to Dec 31, 2024)\n",
    "start_date = datetime(2024, 1, 1, 0, 0, 0)\n",
    "end_date = datetime(2024, 12, 31, 23, 59, 59)\n",
    "time_range_seconds = int((end_date - start_date).total_seconds())\n",
    "\n",
    "timestamps = []\n",
    "for _ in range(NUM_ROWS):\n",
    "    # Select a random number of seconds within the defined range\n",
    "    random_seconds = np.random.randint(0, time_range_seconds)\n",
    "    random_timestamp = start_date + timedelta(seconds=random_seconds)\n",
    "    timestamps.append(random_timestamp)\n",
    "\n",
    "# Sort the timestamps to maintain chronological order (crucial for time-series analysis)\n",
    "timestamps.sort()\n",
    "\n",
    "# --- 2. Generate Base Correlated Data (Normal Operation) ---\n",
    "\n",
    "# We'll use a simple index progression for correlation, as time is now random\n",
    "time_index = np.arange(NUM_ROWS)\n",
    "\n",
    "# Engine RPM (Example range: 800 to 5000 RPM)\n",
    "# Using a smooth sine wave based on index for underlying change, plus noise\n",
    "time_series_base = np.sin(np.linspace(0, 10 * np.pi, NUM_ROWS))\n",
    "rpm_base = 3000 + 2000 * time_series_base + np.random.normal(0, 50, NUM_ROWS)\n",
    "rpm_base = np.clip(rpm_base, 800, 7000)\n",
    "\n",
    "# GPS Speed (Roughly correlated with RPM)\n",
    "speed_base = 0.015 * rpm_base + np.random.normal(0, 5, NUM_ROWS)\n",
    "speed_base = np.clip(speed_base, 0, 120)\n",
    "\n",
    "# Accelerometer Readings\n",
    "accel_x_base = np.random.normal(0, 0.2, NUM_ROWS)\n",
    "accel_y_base = 0.05 * speed_base + np.random.normal(0, 0.3, NUM_ROWS)\n",
    "accel_z_base = 9.8 + np.random.normal(0, 0.1, NUM_ROWS)\n",
    "\n",
    "# Create the initial DataFrame\n",
    "data = {\n",
    "    'Timestamp': timestamps,\n",
    "    'RPM': rpm_base,\n",
    "    'GPS_Speed': speed_base,\n",
    "    'Accel_X': accel_x_base,\n",
    "    'Accel_Y': accel_y_base,\n",
    "    'Accel_Z': accel_z_base,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- 3. Inject Extreme Random Faults (20% of cells) ---\n",
    "\n",
    "sensor_columns = ['RPM', 'GPS_Speed', 'Accel_X', 'Accel_Y', 'Accel_Z']\n",
    "total_cells = NUM_ROWS * len(sensor_columns)\n",
    "num_faults_to_inject = int(total_cells * FAULT_RATE)\n",
    "\n",
    "fault_indices = np.random.choice(total_cells, num_faults_to_inject, replace=False)\n",
    "fault_rows = fault_indices // len(sensor_columns)\n",
    "fault_cols = fault_indices % len(sensor_columns)\n",
    "\n",
    "FAULT_STD_MULTIPLIER = 15 \n",
    "\n",
    "for i in range(num_faults_to_inject):\n",
    "    row = fault_rows[i]\n",
    "    col_name = sensor_columns[fault_cols[i]]\n",
    "\n",
    "    original_value = df.at[row, col_name]\n",
    "    fault_type = np.random.choice(['hike', 'lower'])\n",
    "    std_dev = df[col_name].std()\n",
    "\n",
    "    if fault_type == 'hike':\n",
    "        # Hike the value by 15-30 times the standard deviation\n",
    "        fault_magnitude = np.random.uniform(FAULT_STD_MULTIPLIER, 2 * FAULT_STD_MULTIPLIER) * std_dev\n",
    "        df.at[row, col_name] = original_value + fault_magnitude\n",
    "    else: # 'lower'\n",
    "        fault_magnitude = np.random.uniform(FAULT_STD_MULTIPLIER, 2 * FAULT_STD_MULTIPLIER) * std_dev\n",
    "        new_value = original_value - fault_magnitude\n",
    "\n",
    "        if col_name == 'Accel_Z':\n",
    "            df.at[row, col_name] = np.random.choice([0, 15, -15])\n",
    "        elif col_name == 'RPM' and new_value < 0:\n",
    "            df.at[row, col_name] = np.random.uniform(50, 200)\n",
    "        elif col_name == 'GPS_Speed' and new_value < 0:\n",
    "            df.at[row, col_name] = np.random.uniform(0, 5)\n",
    "        else:\n",
    "            df.at[row, col_name] = np.clip(new_value, 0, new_value) \n",
    "\n",
    "# --- 4. Export to CSV ---\n",
    "file_name = 'sensor_fault_dataset_v2.csv'\n",
    "df.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"Dataset of {NUM_ROWS} rows created successfully!\")\n",
    "print(f\"File saved as: {file_name}\")\n",
    "print(f\"Total faults injected: {num_faults_to_inject} cells ({FAULT_RATE*100:.0f}%)\")\n",
    "print(\"\\nFirst 5 rows of the generated DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fc6b691-6060-4e11-add5-595ab96c8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded raw data from: sensor_fault_dataset_v2.csv\n",
      "âœ… Skipping any data modification/cleaning steps to fully retain all original cell values (including faults).\n",
      "\n",
      "âœ… Preprocessed and scaled dataset saved to: sensor_data_preprocessed_final.csv\n",
      "\n",
      "Quick Verification (Faults must remain as extreme outliers):\n",
      "|      | RPM          | GPS_Speed   | Accel_X   | Accel_Y      | Accel_Z      |\n",
      "|:-----|:-------------|:------------|:----------|:-------------|:-------------|\n",
      "| mean | -7.10543e-18 | 1.77636e-18 | 0         | -5.77316e-18 | -8.88178e-18 |\n",
      "| std  | 1.00025      | 1.00025     | 1.00025   | 1.00025      | 1.00025      |\n",
      "| min  | -0.0883083   | -0.087092   | -5.95547  | -24.2495     | -0.0942075   |\n",
      "| max  | 22.9852      | 24.6146     | 22.0606   | 11.6576      | 18.8671      |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Configuration ---\n",
    "raw_file_name = 'sensor_fault_dataset_v2.csv' # Load the raw file\n",
    "preprocessed_file_name = 'sensor_data_preprocessed_final.csv'\n",
    "sensor_cols = ['RPM', 'GPS_Speed', 'Accel_X', 'Accel_Y', 'Accel_Z']\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(raw_file_name)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    print(f\"âœ… Loaded raw data from: {raw_file_name}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Error: Raw file '{raw_file_name}' not found. Please ensure it exists.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Isolation (No Change to Cell Values) ---\n",
    "# We explicitly skip:\n",
    "# - Dropping rows (no fault removal)\n",
    "# - Imputing missing values (no interpolation/mean substitution)\n",
    "# - Filtering outliers (no modification of extreme fault values)\n",
    "print(\"âœ… Skipping any data modification/cleaning steps to fully retain all original cell values (including faults).\")\n",
    "\n",
    "# --- 3. Feature Scaling (Standardization) ---\n",
    "# This transforms the values based on the data's mean and std dev, \n",
    "# ensuring all sensors contribute equally to anomaly detection, but does NOT remove outliers.\n",
    "\n",
    "X = df[sensor_cols].values\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data (including the faults) and transform it\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=sensor_cols)\n",
    "\n",
    "# --- 4. Final Clean DataFrame and Export ---\n",
    "# Concatenate the Timestamp back with the scaled features\n",
    "df_clean = pd.concat([df[['Timestamp']], df_scaled], axis=1)\n",
    "\n",
    "# Save the final preprocessed data\n",
    "df_clean.to_csv(preprocessed_file_name, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Preprocessed and scaled dataset saved to: {preprocessed_file_name}\")\n",
    "\n",
    "# --- Quick Verification ---\n",
    "print(\"\\nQuick Verification (Faults must remain as extreme outliers):\")\n",
    "scaled_stats = df_clean[sensor_cols].describe().loc[['mean', 'std', 'min', 'max']]\n",
    "print(scaled_stats.to_markdown(numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d12caf-7aec-4ebd-aeb1-c86879632b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in CSV: ['Timestamp', 'RPM', 'GPS_Speed', 'Accel_X', 'Accel_Y', 'Accel_Z']\n",
      "Preprocessed file saved at: preprocessed_data\\vehicle_sensor_data_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "input_file = 'sensor_fault_dataset_v2.csv'  # Replace with your CSV path\n",
    "output_folder = 'preprocessed_data'\n",
    "output_file_name = 'vehicle_sensor_data_preprocessed.csv'\n",
    "\n",
    "# ---------------- CREATE OUTPUT FOLDER ----------------\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "# ---------------- LOAD DATA ----------------\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# ---------------- CHECK COLUMN NAMES ----------------\n",
    "print(\"Columns in CSV:\", df.columns.tolist())\n",
    "\n",
    "# Detect sensor columns automatically (all except Timestamp)\n",
    "timestamp_col = None\n",
    "for col in df.columns:\n",
    "    if 'time' in col.lower():\n",
    "        timestamp_col = col\n",
    "        break\n",
    "\n",
    "if timestamp_col is None:\n",
    "    raise ValueError(\"No timestamp column found!\")\n",
    "\n",
    "sensor_columns = [col for col in df.columns if col != timestamp_col]\n",
    "\n",
    "# ---------------- SAFE PREPROCESSING ----------------\n",
    "df_preprocessed = df.copy()\n",
    "\n",
    "# 1. Convert timestamp to numerical feature (seconds since start)\n",
    "df_preprocessed['Time_sec'] = (pd.to_datetime(df_preprocessed[timestamp_col]) - \n",
    "                               pd.to_datetime(df_preprocessed[timestamp_col].min())).dt.total_seconds()\n",
    "\n",
    "# 2. Handle missing values safely (interpolation)\n",
    "df_preprocessed[sensor_columns] = df_preprocessed[sensor_columns].interpolate(method='linear')\n",
    "\n",
    "# 3. Optional feature engineering (does not modify original sensor values)\n",
    "if 'GPS_Speed' in sensor_columns and 'Engine_RPM' in sensor_columns:\n",
    "    df_preprocessed['Delta_GPS_Speed'] = df_preprocessed['GPS_Speed'].diff().fillna(0)\n",
    "    df_preprocessed['Speed_RPM_Ratio'] = df_preprocessed['GPS_Speed'] / df_preprocessed['Engine_RPM']\n",
    "\n",
    "# ---------------- SAVE PREPROCESSED FILE ----------------\n",
    "df_preprocessed.to_csv(output_path, index=False)\n",
    "print(f\"Preprocessed file saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd75808d-20e6-40aa-a939-d0ca0ec1877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. Total True Faults (Anomaly Rows): 63\n",
      "\n",
      "--- Training PCA Model ---\n",
      "--- Training Isolation Forest Model ---\n",
      "\n",
      "--- Model Evaluation (AUC-ROC) ---\n",
      "\n",
      "=======================================================\n",
      "          MODEL PERFORMANCE COMPARISON (AUC-ROC)\n",
      "=======================================================\n",
      "| Model                      |   AUC-ROC Score |\n",
      "|:---------------------------|----------------:|\n",
      "| Isolation Forest           |        0.996362 |\n",
      "| PCA (Reconstruction Error) |        0.472118 |\n",
      "\n",
      "ðŸ† The Best Performing Model is: **Isolation Forest** (AUC: 0.9964)\n",
      "Proceed with this model for fault attribution (pinpointing the sensor).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "raw_file_name = 'sensor_fault_dataset_v2.csv'\n",
    "preprocessed_file_name = 'vehicle_sensor_data_preprocessed_v2.csv'\n",
    "sensor_cols = ['RPM', 'GPS_Speed', 'Accel_X', 'Accel_Y', 'Accel_Z']\n",
    "OUTLIER_THRESHOLD_STD = 5 # Used to generate ground truth labels\n",
    "\n",
    "# --- 1. Load Data and Generate Ground Truth Labels ---\n",
    "try:\n",
    "    df_raw = pd.read_csv(raw_file_name)\n",
    "    df_clean = pd.read_csv(preprocessed_file_name)\n",
    "    X_scaled = df_clean[sensor_cols].values\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error loading files. Please ensure '{raw_file_name}' and '{preprocessed_file_name}' are in the same folder.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Y_true: 1 if the row contains an anomaly in ANY sensor, 0 otherwise (Ground Truth)\n",
    "Y_true = np.zeros(len(df_raw), dtype=int)\n",
    "for col in sensor_cols:\n",
    "    mean = df_raw[col].mean()\n",
    "    std = df_raw[col].std()\n",
    "    # Flag any reading that is an extreme outlier in the raw data\n",
    "    is_outlier = (df_raw[col] > mean + OUTLIER_THRESHOLD_STD * std) | \\\n",
    "                 (df_raw[col] < mean - OUTLIER_THRESHOLD_STD * std)\n",
    "    Y_true[is_outlier] = 1 \n",
    "\n",
    "print(f\"Data Loaded. Total True Faults (Anomaly Rows): {np.sum(Y_true)}\\n\")\n",
    "\n",
    "# Dictionary to hold the anomaly scores for all models\n",
    "anomaly_scores = {}\n",
    "\n",
    "# --- Helper Function for Normalizing Scores ---\n",
    "def normalize_score(scores):\n",
    "    \"\"\"Scales scores between 0 and 1 for AUC-ROC comparison.\"\"\"\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    return (scores - min_score) / (max_score - min_score)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 2. Model 1: Principal Component Analysis (PCA)\n",
    "# =================================================================\n",
    "print(\"--- Training PCA Model ---\")\n",
    "# Use 3 components, assuming sufficient correlation to explain most variance\n",
    "n_components = 3 \n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_scaled)\n",
    "X_reconstructed = pca.inverse_transform(pca.transform(X_scaled))\n",
    "\n",
    "# Anomaly Score = Sum of squared differences (Reconstruction Error)\n",
    "pca_scores = np.sum((X_scaled - X_reconstructed)**2, axis=1)\n",
    "anomaly_scores['PCA (Reconstruction Error)'] = normalize_score(pca_scores)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 3. Model 2: Isolation Forest (iForest)\n",
    "# =================================================================\n",
    "print(\"--- Training Isolation Forest Model ---\")\n",
    "# Contamination set to 0.20 as we know roughly 20% of rows contain a fault\n",
    "iforest = IsolationForest(contamination=0.20, random_state=42, n_estimators=100)\n",
    "iforest.fit(X_scaled)\n",
    "\n",
    "# Decision function: lower score is more anomalous\n",
    "iforest_raw_scores = iforest.decision_function(X_scaled)\n",
    "\n",
    "# Invert and normalize the score so higher = more anomalous\n",
    "iforest_scores = 1 - iforest_raw_scores\n",
    "anomaly_scores['Isolation Forest'] = normalize_score(iforest_scores)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 4. Evaluation and Model Selection\n",
    "# =================================================================\n",
    "print(\"\\n--- Model Evaluation (AUC-ROC) ---\")\n",
    "results = {}\n",
    "\n",
    "for model_name, scores in anomaly_scores.items():\n",
    "    # AUC-ROC measures the ability to rank anomalies higher than normal data\n",
    "    auc_score = roc_auc_score(Y_true, scores)\n",
    "    results[model_name] = auc_score\n",
    "\n",
    "# Compile and sort results\n",
    "results_df = pd.DataFrame(results.items(), columns=[\"Model\", \"AUC-ROC Score\"]).sort_values(by=\"AUC-ROC Score\", ascending=False)\n",
    "\n",
    "print(\"\\n=======================================================\")\n",
    "print(\"          MODEL PERFORMANCE COMPARISON (AUC-ROC)\")\n",
    "print(\"=======================================================\")\n",
    "print(results_df.to_markdown(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_auc = results_df.iloc[0]['AUC-ROC Score']\n",
    "\n",
    "print(f\"\\nðŸ† The Best Performing Model is: **{best_model_name}** (AUC: {best_auc:.4f})\")\n",
    "print(\"Proceed with this model for fault attribution (pinpointing the sensor).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47550c0-30ab-484a-ac6d-4ebce7a0798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. Total True Faults (Anomaly Rows): 63\n",
      "\n",
      "--- Training PCA Model (Linear) ---\n",
      "--- Training Isolation Forest Model (Tree-based) ---\n",
      "--- Training Gaussian Mixture Model (Probabilistic Density) ---\n",
      "\n",
      "--- Model Evaluation (AUC-ROC) ---\n",
      "\n",
      "=======================================================\n",
      "          MODEL PERFORMANCE COMPARISON (AUC-ROC)\n",
      "=======================================================\n",
      "| Model                      |   AUC-ROC Score |\n",
      "|:---------------------------|----------------:|\n",
      "| Isolation Forest           |        0.996362 |\n",
      "| GMM (Density Estimate)     |        0.920078 |\n",
      "| PCA (Reconstruction Error) |        0.472118 |\n",
      "\n",
      "ðŸ† The Best Performing Model is: **Isolation Forest** (AUC: 0.9964)\n",
      "Proceed with this model for fault attribution.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.mixture import GaussianMixture # New Model for Probabilistic/Density Estimation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "raw_file_name = 'sensor_fault_dataset_v2.csv'\n",
    "preprocessed_file_name = 'vehicle_sensor_data_preprocessed_v2.csv'\n",
    "sensor_cols = ['RPM', 'GPS_Speed', 'Accel_X', 'Accel_Y', 'Accel_Z']\n",
    "OUTLIER_THRESHOLD_STD = 5 \n",
    "\n",
    "# --- 1. Load Data and Generate Ground Truth Labels ---\n",
    "try:\n",
    "    df_raw = pd.read_csv(raw_file_name)\n",
    "    df_clean = pd.read_csv(preprocessed_file_name)\n",
    "    X_scaled = df_clean[sensor_cols].values\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error loading files. Please ensure '{raw_file_name}' and '{preprocessed_file_name}' are in the same folder.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Y_true: 1 if the row contains an anomaly in ANY sensor, 0 otherwise (Ground Truth)\n",
    "Y_true = np.zeros(len(df_raw), dtype=int)\n",
    "for col in sensor_cols:\n",
    "    mean = df_raw[col].mean()\n",
    "    std = df_raw[col].std()\n",
    "    is_outlier = (df_raw[col] > mean + OUTLIER_THRESHOLD_STD * std) | \\\n",
    "                 (df_raw[col] < mean - OUTLIER_THRESHOLD_STD * std)\n",
    "    Y_true[is_outlier] = 1 \n",
    "\n",
    "print(f\"Data Loaded. Total True Faults (Anomaly Rows): {np.sum(Y_true)}\\n\")\n",
    "\n",
    "anomaly_scores = {}\n",
    "\n",
    "# --- Helper Function for Normalizing Scores ---\n",
    "def normalize_score(scores):\n",
    "    \"\"\"Scales scores between 0 and 1 (higher = more anomalous).\"\"\"\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    # Handle case where all scores might be identical (unlikely)\n",
    "    if max_score == min_score:\n",
    "        return np.zeros_like(scores)\n",
    "    return (scores - min_score) / (max_score - min_score)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 2. Model 1: Principal Component Analysis (PCA)\n",
    "# =================================================================\n",
    "print(\"--- Training PCA Model (Linear) ---\")\n",
    "n_components = 3 \n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_scaled)\n",
    "X_reconstructed = pca.inverse_transform(pca.transform(X_scaled))\n",
    "\n",
    "# Anomaly Score = Reconstruction Error (Higher is worse)\n",
    "pca_scores = np.sum((X_scaled - X_reconstructed)**2, axis=1)\n",
    "anomaly_scores['PCA (Reconstruction Error)'] = normalize_score(pca_scores)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 3. Model 2: Isolation Forest (iForest)\n",
    "# =================================================================\n",
    "print(\"--- Training Isolation Forest Model (Tree-based) ---\")\n",
    "iforest = IsolationForest(contamination=0.20, random_state=42, n_estimators=100)\n",
    "iforest.fit(X_scaled)\n",
    "\n",
    "# Decision function: lower score is more anomalous\n",
    "iforest_raw_scores = iforest.decision_function(X_scaled)\n",
    "\n",
    "# Anomaly Score: Invert and normalize (Higher is worse)\n",
    "iforest_scores = 1 - iforest_raw_scores\n",
    "anomaly_scores['Isolation Forest'] = normalize_score(iforest_scores)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 4. Model 3: Gaussian Mixture Model (GMM) - Probabilistic Density\n",
    "# =================================================================\n",
    "print(\"--- Training Gaussian Mixture Model (Probabilistic Density) ---\")\n",
    "# GMM is a probabilistic density estimator. Low probability = anomaly.\n",
    "# Use 2 components: one for \"normal\" driving, one for \"idle/low speed\" or similar behavior patterns.\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
    "gmm.fit(X_scaled)\n",
    "\n",
    "# score_samples returns the log-likelihood (log-probability) of each sample.\n",
    "# Higher log-likelihood means higher probability (more normal).\n",
    "log_likelihoods = gmm.score_samples(X_scaled)\n",
    "\n",
    "# Anomaly Score: Invert the log-likelihood (Lower log-likelihood = Higher anomaly score)\n",
    "gmm_scores = -log_likelihoods\n",
    "anomaly_scores['GMM (Density Estimate)'] = normalize_score(gmm_scores)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "## 5. Evaluation and Model Selection\n",
    "# =================================================================\n",
    "print(\"\\n--- Model Evaluation (AUC-ROC) ---\")\n",
    "results = {}\n",
    "\n",
    "for model_name, scores in anomaly_scores.items():\n",
    "    auc_score = roc_auc_score(Y_true, scores)\n",
    "    results[model_name] = auc_score\n",
    "\n",
    "# Compile and sort results\n",
    "results_df = pd.DataFrame(results.items(), columns=[\"Model\", \"AUC-ROC Score\"]).sort_values(by=\"AUC-ROC Score\", ascending=False)\n",
    "\n",
    "print(\"\\n=======================================================\")\n",
    "print(\"          MODEL PERFORMANCE COMPARISON (AUC-ROC)\")\n",
    "print(\"=======================================================\")\n",
    "print(results_df.to_markdown(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_auc = results_df.iloc[0]['AUC-ROC Score']\n",
    "\n",
    "print(f\"\\nðŸ† The Best Performing Model is: **{best_model_name}** (AUC: {best_auc:.4f})\")\n",
    "print(\"Proceed with this model for fault attribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10a465c-f5ee-463a-ad28-6e2f2468562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Output folder 'model_data' ensured.\n",
      "âœ… Training data saved to: model_data\\X_train_scaled.csv\n",
      "âœ… Testing data saved to: model_data\\X_test_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split # Assuming you need to run the split\n",
    "\n",
    "# --- Contextual setup (Defining variables needed by the snippet) ---\n",
    "\n",
    "# Replace with your actual sensor names\n",
    "sensor_cols = ['RPM', 'GPS_Speed', 'Accel_X', 'Accel_Y', 'Accel_Z'] \n",
    "OUTPUT_FOLDER = 'model_data' \n",
    "\n",
    "# Create placeholder data for X_train, X_test, indices (2000 total rows, 80/20 split)\n",
    "total_rows = 2000\n",
    "train_rows = int(total_rows * 0.8)\n",
    "test_rows = total_rows - train_rows\n",
    "\n",
    "# Dummy scaled features (5 columns)\n",
    "X_scaled_all = np.random.rand(total_rows, len(sensor_cols))\n",
    "indices_all = np.arange(total_rows)\n",
    "\n",
    "# The necessary train_test_split that creates the input variables\n",
    "X_train, X_test, idx_train, idx_test = train_test_split(\n",
    "    X_scaled_all, indices_all, \n",
    "    test_size=0.20, \n",
    "    random_state=42,\n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "print(f\"ðŸ“ Output folder '{OUTPUT_FOLDER}' ensured.\")\n",
    "\n",
    "# --- Your Code Snippet (The part that saves the files) ---\n",
    "\n",
    "df_train = pd.DataFrame(X_train, columns=sensor_cols, index=idx_train)\n",
    "df_test = pd.DataFrame(X_test, columns=sensor_cols, index=idx_test)\n",
    "\n",
    "train_file_path = os.path.join(OUTPUT_FOLDER, 'X_train_scaled.csv')\n",
    "test_file_path = os.path.join(OUTPUT_FOLDER, 'X_test_scaled.csv')\n",
    "\n",
    "df_train.to_csv(train_file_path)\n",
    "df_test.to_csv(test_file_path)\n",
    "\n",
    "print(f\"âœ… Training data saved to: {train_file_path}\")\n",
    "print(f\"âœ… Testing data saved to: {test_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2cb52d-8092-403e-8530-8b6f72875615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Proportionality Constants (from training data):\n",
      "k1 (GPS_Speed factor): 67.21\n",
      "k2 (Engine_RPM factor): 5562.69\n",
      "\n",
      "âœ… Isolation Forest Model Trained Successfully.\n",
      "Anomaly Threshold Score: 0.0000\n",
      "\n",
      "âœ… Anomaly Report Generated Successfully and saved to 'anomaly_report_combined.csv'.\n",
      "\n",
      "--- Sample Anomalous Records ---\n",
      "               Timestamp Anomaly         Faulty_Sensors  Actual_GPS_Speed  \\\n",
      "4004 2025-01-01 01:06:44     Yes             Engine_RPM         49.172595   \n",
      "4033 2025-01-01 01:07:13     Yes  GPS_Speed, Engine_RPM          6.817570   \n",
      "4058 2025-01-01 01:07:38     Yes             Engine_RPM         53.547114   \n",
      "4081 2025-01-01 01:08:01     Yes  GPS_Speed, Engine_RPM         25.315223   \n",
      "4120 2025-01-01 01:08:40     Yes  GPS_Speed, Engine_RPM         45.685339   \n",
      "\n",
      "      Expected_GPS_Speed  Actual_Engine_RPM  Expected_Engine_RPM  \n",
      "4004           49.141303        2221.335262          4067.429651  \n",
      "4033          103.953640        1852.129997          8604.251367  \n",
      "4058           61.111621        2581.608069          5058.213878  \n",
      "4081           60.110900        1987.069658          4975.384163  \n",
      "4120           78.528893        2417.847511          6499.842925  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- CONFIGURATION & PLACEHOLDER DATA SETUP ---\n",
    "# NOTE: Replace this placeholder data loading with your actual data file\n",
    "data_size = 5000\n",
    "np.random.seed(42)\n",
    "df_raw = pd.DataFrame({\n",
    "    'Timestamp': pd.to_datetime(pd.date_range('2025-01-01', periods=data_size, freq='s')),\n",
    "    'Accel_X': np.random.normal(0, 0.5, data_size),\n",
    "    'Accel_Y': np.random.normal(0, 0.5, data_size),\n",
    "    'Accel_Z': np.random.normal(9.8, 0.1, data_size), # Gravity is around 9.8 m/s^2\n",
    "    'GPS_Speed': np.random.normal(30, 10, data_size),\n",
    "    'Engine_RPM': np.random.normal(2500, 300, data_size)\n",
    "})\n",
    "sensor_cols = ['Accel_X', 'Accel_Y', 'Accel_Z', 'GPS_Speed', 'Engine_RPM']\n",
    "CONTAMINATION = 0.05 # Assume 5% of data is anomalous\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Introduce deliberate anomalies for testing the detection logic\n",
    "df_raw.loc[1630, 'GPS_Speed'] = 450.0  # Extreme GPS_Speed anomaly\n",
    "df_raw.loc[2500, 'Engine_RPM'] = 50.0   # Extreme RPM drop\n",
    "df_raw.loc[3000, 'Accel_X'] = 100.0   # Extreme Accel spike\n",
    "\n",
    "# Use 80% of data for training the IF model\n",
    "df_train, df_test = train_test_split(\n",
    "    df_raw, test_size=0.2, random_state=RANDOM_STATE, shuffle=False\n",
    ")\n",
    "\n",
    "# --- STEP 1: CALCULATE PROPORTIONALITY CONSTANTS (k1, k2) ---\n",
    "\n",
    "# Calculate the Magnitude of Net Acceleration (removing static gravity component)\n",
    "# This is the acceleration experienced by the vehicle.\n",
    "accel_mag_train = np.sqrt(\n",
    "    df_train['Accel_X']**2 + df_train['Accel_Y']**2 + (df_train['Accel_Z'] - 9.8)**2 \n",
    ")\n",
    "\n",
    "# Estimate k1 and k2 (proportionality factors) from the training data by calculating the mean ratio.\n",
    "# Add a small epsilon to prevent division by zero.\n",
    "epsilon = 1e-6 \n",
    "k1 = (df_train['GPS_Speed'] / (accel_mag_train + epsilon)).mean()\n",
    "k2 = (df_train['Engine_RPM'] / (accel_mag_train + epsilon)).mean()\n",
    "\n",
    "print(f\"Calculated Proportionality Constants (from training data):\")\n",
    "print(f\"k1 (GPS_Speed factor): {k1:.2f}\")\n",
    "print(f\"k2 (Engine_RPM factor): {k2:.2f}\")\n",
    "\n",
    "# --- STEP 2: TRAIN ISOLATION FOREST MODEL ---\n",
    "\n",
    "# 1. Prepare and scale the training data\n",
    "X_train_raw = df_train[sensor_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "# 2. Train the model\n",
    "iforest_model = IsolationForest(\n",
    "    contamination=CONTAMINATION, \n",
    "    random_state=RANDOM_STATE, \n",
    "    n_estimators=100\n",
    ")\n",
    "iforest_model.fit(X_train_scaled)\n",
    "\n",
    "# 3. Calculate the Anomaly Threshold\n",
    "train_scores = iforest_model.decision_function(X_train_scaled)\n",
    "# The threshold is the score at the contamination percentile (e.g., 5th percentile)\n",
    "anomaly_threshold = np.percentile(train_scores, CONTAMINATION * 100) \n",
    "\n",
    "print(\"\\nâœ… Isolation Forest Model Trained Successfully.\")\n",
    "print(f\"Anomaly Threshold Score: {anomaly_threshold:.4f}\")\n",
    "\n",
    "# --- STEP 3: ANOMALY DETECTION AND FAULT FLAGGING ON TEST DATA ---\n",
    "\n",
    "# Prepare data for prediction (using the 20% test set)\n",
    "df_results = df_test.copy()\n",
    "X_test_scaled = scaler.transform(df_results[sensor_cols].values)\n",
    "\n",
    "# A. Isolation Forest Detection (Overall Anomaly)\n",
    "df_results['IF_Anomaly_Score'] = iforest_model.decision_function(X_test_scaled)\n",
    "\n",
    "# FIXED: Compare the score to the calculated anomaly_threshold\n",
    "df_results['Anomaly'] = np.where(\n",
    "    df_results['IF_Anomaly_Score'] < anomaly_threshold, \n",
    "    'Yes', \n",
    "    'No'\n",
    ") \n",
    "\n",
    "\n",
    "# B. Physics-Based Correlation Check (Fault Flagging)\n",
    "# If the difference is > 50% of the actual value, flag it.\n",
    "DEVIATION_TOLERANCE = 0.5 \n",
    "\n",
    "def get_expected_and_fault(row):\n",
    "    \"\"\"Computes expected values and flags faults based on deviation.\"\"\"\n",
    "    \n",
    "    # Calculate Magnitude of Net Acceleration\n",
    "    accel_mag = np.sqrt(\n",
    "        row['Accel_X']**2 + row['Accel_Y']**2 + (row['Accel_Z'] - 9.8)**2\n",
    "    )\n",
    "    \n",
    "    # Compute Expected Values using the calculated constants\n",
    "    expected_gps = k1 * accel_mag\n",
    "    expected_rpm = k2 * accel_mag\n",
    "    \n",
    "    faulty_sensors = []\n",
    "    \n",
    "    # Check GPS_Speed deviation: Flag if difference > 50% of the actual speed\n",
    "    if row['GPS_Speed'] > 1: # Avoid division by near-zero\n",
    "        gps_dev_ratio = np.abs(row['GPS_Speed'] - expected_gps) / row['GPS_Speed']\n",
    "        if gps_dev_ratio > DEVIATION_TOLERANCE:\n",
    "            faulty_sensors.append('GPS_Speed')\n",
    "        \n",
    "    # Check Engine_RPM deviation: Flag if difference > 50% of the actual RPM\n",
    "    if row['Engine_RPM'] > 1: # Avoid division by near-zero\n",
    "        rpm_dev_ratio = np.abs(row['Engine_RPM'] - expected_rpm) / row['Engine_RPM']\n",
    "        if rpm_dev_ratio > DEVIATION_TOLERANCE:\n",
    "            faulty_sensors.append('Engine_RPM')\n",
    "        \n",
    "    # Logic: If IF model flagged it but GPS/RPM were normal, the fault is likely in Accel sensors\n",
    "    if row['Anomaly'] == 'Yes' and not faulty_sensors:\n",
    "        faulty_sensors.append('Accel_Sensor(s)')\n",
    "\n",
    "    return expected_gps, expected_rpm, ', '.join(faulty_sensors)\n",
    "\n",
    "# Apply the fault checking function\n",
    "results = df_results.apply(get_expected_and_fault, axis=1, result_type='expand')\n",
    "df_results['Expected_GPS_Speed'], df_results['Expected_Engine_RPM'], df_results['Faulty_Sensors'] = results[0], results[1], results[2]\n",
    "\n",
    "\n",
    "# --- STEP 4: FINAL OUTPUT ---\n",
    "\n",
    "final_columns = [\n",
    "    'Timestamp', \n",
    "    'Anomaly', \n",
    "    'Faulty_Sensors', \n",
    "    'GPS_Speed', \n",
    "    'Expected_GPS_Speed', \n",
    "    'Engine_RPM', \n",
    "    'Expected_Engine_RPM'\n",
    "]\n",
    "df_output = df_results[final_columns]\n",
    "df_output = df_output.rename(columns={'GPS_Speed': 'Actual_GPS_Speed', 'Engine_RPM': 'Actual_Engine_RPM'})\n",
    "\n",
    "output_file = 'anomaly_report_combined.csv'\n",
    "df_output.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Anomaly Report Generated Successfully and saved to '{output_file}'.\")\n",
    "print(\"\\n--- Sample Anomalous Records ---\")\n",
    "# Display records flagged as anomalous\n",
    "print(df_output[df_output['Anomaly'] == 'Yes'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c0f94f-52b3-48ac-8b54-fd8f7cbe4757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Fault Detection System (Training Model)...\n",
      "âœ… Model Setup Complete.\n",
      "k1 (GPS_Speed factor): 67.09, k2 (Engine_RPM factor): 5564.21\n",
      "Anomaly Threshold Score: 0.0000\n",
      "\n",
      "\n",
      "################ STARTING BATCH TEST EVALUATION ################\n",
      "Loaded 400 records for testing.\n",
      "\n",
      "======== Row Index 1600 (Test Sample 1) ========\n",
      "\n",
      "==================================================\n",
      "      ðŸš¨ FAULT DETECTED ðŸš¨     \n",
      "==================================================\n",
      "Input Reading:\n",
      "  Accel_X: 0.05, Accel_Y: 0.36, Accel_Z: 0.76\n",
      "  GPS_Speed: 0.11, Engine_RPM: 0.41\n",
      "--------------------------------------------------\n",
      "Model Anomaly Score: -0.1239 (ANOMALY)\n",
      "\n",
      "FAULT ATTRIBUTION (Physics Check):\n",
      "-> Faulty Sensor(s) Identified: Accel_Sensor(s)\n",
      "   Expected GPS_Speed: 606.91 (Actual: 0.11)\n",
      "   Expected Engine_RPM: 50334.79 (Actual: 0.41)\n",
      "==================================================\n",
      "\n",
      "======== Row Index 1601 (Test Sample 2) ========\n",
      "\n",
      "==================================================\n",
      "      ðŸš¨ FAULT DETECTED ðŸš¨     \n",
      "==================================================\n",
      "Input Reading:\n",
      "  Accel_X: 0.39, Accel_Y: 0.49, Accel_Z: 0.87\n",
      "  GPS_Speed: 0.62, Engine_RPM: 0.73\n",
      "--------------------------------------------------\n",
      "Model Anomaly Score: -0.1399 (ANOMALY)\n",
      "\n",
      "FAULT ATTRIBUTION (Physics Check):\n",
      "-> Faulty Sensor(s) Identified: Accel_Sensor(s)\n",
      "   Expected GPS_Speed: 600.43 (Actual: 0.62)\n",
      "   Expected Engine_RPM: 49797.67 (Actual: 0.73)\n",
      "==================================================\n",
      "\n",
      "======== Row Index 1602 (Test Sample 3) ========\n",
      "\n",
      "==================================================\n",
      "      ðŸš¨ FAULT DETECTED ðŸš¨     \n",
      "==================================================\n",
      "Input Reading:\n",
      "  Accel_X: 0.74, Accel_Y: 0.78, Accel_Z: 0.29\n",
      "  GPS_Speed: 0.29, Engine_RPM: 0.78\n",
      "--------------------------------------------------\n",
      "Model Anomaly Score: -0.1597 (ANOMALY)\n",
      "\n",
      "FAULT ATTRIBUTION (Physics Check):\n",
      "-> Faulty Sensor(s) Identified: Accel_Sensor(s)\n",
      "   Expected GPS_Speed: 642.09 (Actual: 0.29)\n",
      "   Expected Engine_RPM: 53252.40 (Actual: 0.78)\n",
      "==================================================\n",
      "\n",
      "======== Row Index 1603 (Test Sample 4) ========\n",
      "\n",
      "==================================================\n",
      "      ðŸš¨ FAULT DETECTED ðŸš¨     \n",
      "==================================================\n",
      "Input Reading:\n",
      "  Accel_X: 0.79, Accel_Y: 0.77, Accel_Z: 0.89\n",
      "  GPS_Speed: 0.56, Engine_RPM: 0.89\n",
      "--------------------------------------------------\n",
      "Model Anomaly Score: -0.1636 (ANOMALY)\n",
      "\n",
      "FAULT ATTRIBUTION (Physics Check):\n",
      "-> Faulty Sensor(s) Identified: Accel_Sensor(s)\n",
      "   Expected GPS_Speed: 602.18 (Actual: 0.56)\n",
      "   Expected Engine_RPM: 49942.42 (Actual: 0.89)\n",
      "==================================================\n",
      "\n",
      "======== Row Index 1604 (Test Sample 5) ========\n",
      "\n",
      "==================================================\n",
      "      ðŸš¨ FAULT DETECTED ðŸš¨     \n",
      "==================================================\n",
      "Input Reading:\n",
      "  Accel_X: 0.45, Accel_Y: 0.88, Accel_Z: 0.78\n",
      "  GPS_Speed: 0.27, Engine_RPM: 0.41\n",
      "--------------------------------------------------\n",
      "Model Anomaly Score: -0.1539 (ANOMALY)\n",
      "\n",
      "FAULT ATTRIBUTION (Physics Check):\n",
      "-> Faulty Sensor(s) Identified: Accel_Sensor(s)\n",
      "   Expected GPS_Speed: 608.66 (Actual: 0.27)\n",
      "   Expected Engine_RPM: 50479.82 (Actual: 0.41)\n",
      "==================================================\n",
      "\n",
      "################ BATCH TEST EVALUATION COMPLETE ################\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- GLOBAL CONFIGURATION AND DATA SETUP ---\n",
    "# NOTE: Using placeholder data to define global variables (k1, k2, scaler, etc.)\n",
    "# Replace this with your actual data loading if necessary for the setup.\n",
    "data_size = 5000\n",
    "np.random.seed(42)\n",
    "df_raw = pd.DataFrame({\n",
    "    'Timestamp': pd.to_datetime(pd.date_range('2025-01-01', periods=data_size, freq='s')),\n",
    "    'Accel_X': np.random.normal(0, 0.5, data_size),\n",
    "    'Accel_Y': np.random.normal(0, 0.5, data_size),\n",
    "    'Accel_Z': np.random.normal(9.8, 0.1, data_size),\n",
    "    'GPS_Speed': np.random.normal(30, 10, data_size),\n",
    "    'Engine_RPM': np.random.normal(2500, 300, data_size)\n",
    "})\n",
    "sensor_cols = ['Accel_X', 'Accel_Y', 'Accel_Z', 'GPS_Speed', 'Engine_RPM']\n",
    "CONTAMINATION = 0.05\n",
    "RANDOM_STATE = 42\n",
    "DEVIATION_TOLERANCE = 0.5 # 50% deviation\n",
    "\n",
    "# Global variables for model, scaler, and constants\n",
    "iforest_model = None\n",
    "scaler = None\n",
    "k1 = None\n",
    "k2 = None\n",
    "anomaly_threshold = None\n",
    "\n",
    "# --- FILE CONFIGURATION ---\n",
    "TEST_FILE_NAME = 'X_test_scaled_v2.csv' \n",
    "\n",
    "# =========================================================================\n",
    "# 1. SETUP FUNCTION (TRAINS MODEL AND CALCULATES CONSTANTS)\n",
    "# =========================================================================\n",
    "\n",
    "def setup_model():\n",
    "    \"\"\"Trains the model, fits the scaler, and calculates k1/k2 constants.\"\"\"\n",
    "    global iforest_model, scaler, k1, k2, anomaly_threshold\n",
    "\n",
    "    print(\"ðŸš€ Initializing Fault Detection System (Training Model)...\")\n",
    "\n",
    "    # Use 80% of data for training\n",
    "    df_train, _ = train_test_split(\n",
    "        df_raw, test_size=0.2, random_state=RANDOM_STATE, shuffle=False\n",
    "    )\n",
    "\n",
    "    # --- Calculate Proportionality Constants (k1, k2) ---\n",
    "    accel_mag_train = np.sqrt(\n",
    "        df_train['Accel_X']**2 + df_train['Accel_Y']**2 + (df_train['Accel_Z'] - 9.8)**2\n",
    "    )\n",
    "    epsilon = 1e-6\n",
    "    k1 = (df_train['GPS_Speed'] / (accel_mag_train + epsilon)).mean()\n",
    "    k2 = (df_train['Engine_RPM'] / (accel_mag_train + epsilon)).mean()\n",
    "\n",
    "    # --- Train Isolation Forest Model ---\n",
    "    X_train_raw = df_train[sensor_cols].values\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "    iforest_model = IsolationForest(contamination=CONTAMINATION, random_state=RANDOM_STATE, n_estimators=100)\n",
    "    iforest_model.fit(X_train_scaled)\n",
    "\n",
    "    # Calculate Anomaly Threshold\n",
    "    train_scores = iforest_model.decision_function(X_train_scaled)\n",
    "    anomaly_threshold = np.percentile(train_scores, CONTAMINATION * 100)\n",
    "\n",
    "    print(\"âœ… Model Setup Complete.\")\n",
    "    print(f\"k1 (GPS_Speed factor): {k1:.2f}, k2 (Engine_RPM factor): {k2:.2f}\")\n",
    "    print(f\"Anomaly Threshold Score: {anomaly_threshold:.4f}\")\n",
    "\n",
    "# =========================================================================\n",
    "# 2. PREDICTION INTERFACE FUNCTION (CORE LOGIC)\n",
    "# =========================================================================\n",
    "\n",
    "def predict_fault_on_new_reading(accel_x, accel_y, accel_z, gps_speed, engine_rpm):\n",
    "    \"\"\"\n",
    "    Takes 5 sensor readings, runs both IF detection and physics-based attribution,\n",
    "    and prints a clear report.\n",
    "    \"\"\"\n",
    "    global iforest_model, scaler, k1, k2, anomaly_threshold, sensor_cols\n",
    "    \n",
    "    if iforest_model is None:\n",
    "        return\n",
    "\n",
    "    # A. Format and Scale the raw input for IF model\n",
    "    raw_input_data = {\n",
    "        'Accel_X': accel_x, 'Accel_Y': accel_y, 'Accel_Z': accel_z, \n",
    "        'GPS_Speed': gps_speed, 'Engine_RPM': engine_rpm\n",
    "    }\n",
    "    df_new_input = pd.DataFrame([raw_input_data], columns=sensor_cols)\n",
    "    X_new_raw = df_new_input.values\n",
    "    X_new_scaled = scaler.transform(X_new_raw)\n",
    "\n",
    "    # B. Isolation Forest Detection\n",
    "    anomaly_score = iforest_model.decision_function(X_new_scaled)[0]\n",
    "    is_anomaly = (anomaly_score < anomaly_threshold)\n",
    "\n",
    "    # C. Physics-Based Correlation Check (Fault Attribution)\n",
    "    faulty_sensors = []\n",
    "    \n",
    "    # Calculate Magnitude of Net Acceleration\n",
    "    accel_mag = np.sqrt(\n",
    "        accel_x**2 + accel_y**2 + (accel_z - 9.8)**2\n",
    "    )\n",
    "    \n",
    "    # Compute Expected Values\n",
    "    expected_gps = k1 * accel_mag\n",
    "    expected_rpm = k2 * accel_mag\n",
    "    \n",
    "    # Check GPS_Speed deviation\n",
    "    if gps_speed > 1:\n",
    "        gps_dev_ratio = np.abs(gps_speed - expected_gps) / gps_speed\n",
    "        if gps_dev_ratio > DEVIATION_TOLERANCE:\n",
    "            faulty_sensors.append('GPS_Speed')\n",
    "        \n",
    "    # Check Engine_RPM deviation\n",
    "    if engine_rpm > 1:\n",
    "        rpm_dev_ratio = np.abs(engine_rpm - expected_rpm) / engine_rpm\n",
    "        if rpm_dev_ratio > DEVIATION_TOLERANCE:\n",
    "            faulty_sensors.append('Engine_RPM')\n",
    "        \n",
    "    # Final Attribution Logic\n",
    "    if is_anomaly and not faulty_sensors:\n",
    "        faulty_sensors.append('Accel_Sensor(s)')\n",
    "    elif not is_anomaly and faulty_sensors:\n",
    "        pass # Keep the flags\n",
    "\n",
    "    # D. Print Report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"      {'ðŸš¨ FAULT DETECTED ðŸš¨' if is_anomaly or faulty_sensors else 'ðŸŸ¢ NORMAL READING ðŸŸ¢'}     \")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Input Reading:\")\n",
    "    print(f\"  Accel_X: {accel_x:.2f}, Accel_Y: {accel_y:.2f}, Accel_Z: {accel_z:.2f}\")\n",
    "    print(f\"  GPS_Speed: {gps_speed:.2f}, Engine_RPM: {engine_rpm:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Model Anomaly Score: {anomaly_score:.4f} ({'ANOMALY' if is_anomaly else 'NORMAL'})\")\n",
    "    \n",
    "    if faulty_sensors:\n",
    "        print(f\"\\nFAULT ATTRIBUTION (Physics Check):\")\n",
    "        print(f\"-> Faulty Sensor(s) Identified: {', '.join(faulty_sensors)}\")\n",
    "        print(f\"   Expected GPS_Speed: {expected_gps:.2f} (Actual: {gps_speed:.2f})\")\n",
    "        print(f\"   Expected Engine_RPM: {expected_rpm:.2f} (Actual: {engine_rpm:.2f})\")\n",
    "    elif is_anomaly:\n",
    "        print(f\"\\nATTRIBUTION:\")\n",
    "        print(\"-> ANOMALY detected by Isolation Forest. Suspect: Subtle correlation or Accel_Sensor(s) fault.\")\n",
    "    else:\n",
    "        print(\"\\nATTRIBUTION: All sensors and correlations are within normal limits.\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# =========================================================================\n",
    "# 3. TESTING LOOP FUNCTION\n",
    "# =========================================================================\n",
    "\n",
    "def test_model_on_test_data(file_name):\n",
    "    \"\"\"Loads the test data file and runs the prediction function on sample rows.\"\"\"\n",
    "    print(\"\\n\\n################ STARTING BATCH TEST EVALUATION ################\")\n",
    "    \n",
    "    try:\n",
    "        # Load the test data file\n",
    "        # The first column is often an index saved by pandas, so we use index_col=0\n",
    "        df_test = pd.read_csv(file_name, index_col=0) \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Test data file not found at '{file_name}'. Please ensure the file is in the same folder.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}.\")\n",
    "        return\n",
    "\n",
    "    # Check for initialization before proceeding\n",
    "    if iforest_model is None or k1 is None:\n",
    "         print(\"âŒ Setup Error: Model or constants are not initialized. Run setup_model() first.\")\n",
    "         return\n",
    "\n",
    "    print(f\"Loaded {len(df_test)} records for testing.\")\n",
    "    \n",
    "    # Run the prediction function for a few sample rows (first 5)\n",
    "    test_indices = [0, 1, 2, 3, 4] \n",
    "    test_indices = sorted(list(set([i for i in test_indices if i < len(df_test)])))\n",
    "\n",
    "    for i in test_indices:\n",
    "        row = df_test.iloc[i]\n",
    "        \n",
    "        print(f\"\\n======== Row Index {row.name} (Test Sample {i+1}) ========\")\n",
    "        \n",
    "        # Call the interactive prediction function\n",
    "        predict_fault_on_new_reading(\n",
    "            accel_x=row['Accel_X'], \n",
    "            accel_y=row['Accel_Y'], \n",
    "            accel_z=row['Accel_Z'], \n",
    "            gps_speed=row['GPS_Speed'], \n",
    "            engine_rpm=row['RPM']\n",
    "        )\n",
    "    \n",
    "    print(\"\\n################ BATCH TEST EVALUATION COMPLETE ################\")\n",
    "\n",
    "# =========================================================================\n",
    "# 4. EXECUTION\n",
    "# =========================================================================\n",
    "\n",
    "# Step 1: Initialize and train the model\n",
    "setup_model()\n",
    "\n",
    "# Step 2: Run the testing loop against the test file\n",
    "test_model_on_test_data(TEST_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fc3162-5b2f-40b7-b1f7-7f6e8bea0d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Fault Detection System (Training Model)...\n",
      "âœ… Model Setup Complete.\n",
      "\n",
      "\n",
      "################ STARTING BATCH TEST EVALUATION ################\n",
      "Loaded 400 records for testing.\n",
      "\n",
      "==================================================\n",
      "ðŸš¨ FAULT DETECTED in Row No: 1600 ðŸš¨\n",
      "==================================================\n",
      "Faulty Sensor(s) Identified: **Accel_Sensor(s)**\n",
      "-> **ACCEL_SENSOR(S)** sensor has fault in it in row no **1600**\n",
      "--------------------------------------------------\n",
      "IF Anomaly Score: -0.1239 (ANOMALY)\n",
      "Expected GPS: 606.91 (Actual: 0.11)\n",
      "Expected RPM: 50334.79 (Actual: 0.41)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ðŸš¨ FAULT DETECTED in Row No: 1601 ðŸš¨\n",
      "==================================================\n",
      "Faulty Sensor(s) Identified: **Accel_Sensor(s)**\n",
      "-> **ACCEL_SENSOR(S)** sensor has fault in it in row no **1601**\n",
      "--------------------------------------------------\n",
      "IF Anomaly Score: -0.1399 (ANOMALY)\n",
      "Expected GPS: 600.43 (Actual: 0.62)\n",
      "Expected RPM: 49797.67 (Actual: 0.73)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ðŸš¨ FAULT DETECTED in Row No: 1602 ðŸš¨\n",
      "==================================================\n",
      "Faulty Sensor(s) Identified: **Accel_Sensor(s)**\n",
      "-> **ACCEL_SENSOR(S)** sensor has fault in it in row no **1602**\n",
      "--------------------------------------------------\n",
      "IF Anomaly Score: -0.1597 (ANOMALY)\n",
      "Expected GPS: 642.09 (Actual: 0.29)\n",
      "Expected RPM: 53252.40 (Actual: 0.78)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ðŸš¨ FAULT DETECTED in Row No: 1603 ðŸš¨\n",
      "==================================================\n",
      "Faulty Sensor(s) Identified: **Accel_Sensor(s)**\n",
      "-> **ACCEL_SENSOR(S)** sensor has fault in it in row no **1603**\n",
      "--------------------------------------------------\n",
      "IF Anomaly Score: -0.1636 (ANOMALY)\n",
      "Expected GPS: 602.18 (Actual: 0.56)\n",
      "Expected RPM: 49942.42 (Actual: 0.89)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ðŸš¨ FAULT DETECTED in Row No: 1604 ðŸš¨\n",
      "==================================================\n",
      "Faulty Sensor(s) Identified: **Accel_Sensor(s)**\n",
      "-> **ACCEL_SENSOR(S)** sensor has fault in it in row no **1604**\n",
      "--------------------------------------------------\n",
      "IF Anomaly Score: -0.1539 (ANOMALY)\n",
      "Expected GPS: 608.66 (Actual: 0.27)\n",
      "Expected RPM: 50479.82 (Actual: 0.41)\n",
      "==================================================\n",
      "\n",
      "################ BATCH TEST EVALUATION COMPLETE ################\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- GLOBAL CONFIGURATION AND DATA SETUP ---\n",
    "# NOTE: Using placeholder data to define global variables (k1, k2, scaler, etc.)\n",
    "data_size = 5000\n",
    "np.random.seed(42)\n",
    "df_raw = pd.DataFrame({\n",
    "    'Timestamp': pd.to_datetime(pd.date_range('2025-01-01', periods=data_size, freq='s')),\n",
    "    'Accel_X': np.random.normal(0, 0.5, data_size),\n",
    "    'Accel_Y': np.random.normal(0, 0.5, data_size),\n",
    "    'Accel_Z': np.random.normal(9.8, 0.1, data_size),\n",
    "    'GPS_Speed': np.random.normal(30, 10, data_size),\n",
    "    'Engine_RPM': np.random.normal(2500, 300, data_size)\n",
    "})\n",
    "# !! CRITICAL FIX !! Ensure these sensor names EXACTLY match the columns in your CSV file\n",
    "sensor_cols = ['Accel_X', 'Accel_Y', 'Accel_Z', 'GPS_Speed', 'Engine_RPM'] \n",
    "\n",
    "CONTAMINATION = 0.05\n",
    "RANDOM_STATE = 42\n",
    "DEVIATION_TOLERANCE = 0.5 # 50% deviation\n",
    "\n",
    "# Global variables for model, scaler, and constants\n",
    "iforest_model = None\n",
    "scaler = None\n",
    "k1 = None\n",
    "k2 = None\n",
    "anomaly_threshold = None\n",
    "\n",
    "# --- FILE CONFIGURATION ---\n",
    "TEST_FILE_NAME = 'X_test_scaled_v2.csv' \n",
    "\n",
    "# =========================================================================\n",
    "# 1. SETUP FUNCTION (TRAINS MODEL AND CALCULATES CONSTANTS)\n",
    "# =========================================================================\n",
    "\n",
    "def setup_model():\n",
    "    \"\"\"Trains the model, fits the scaler, and calculates k1/k2 constants.\"\"\"\n",
    "    global iforest_model, scaler, k1, k2, anomaly_threshold\n",
    "\n",
    "    print(\"ðŸš€ Initializing Fault Detection System (Training Model)...\")\n",
    "\n",
    "    df_train, _ = train_test_split(\n",
    "        df_raw, test_size=0.2, random_state=RANDOM_STATE, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Calculate Proportionality Constants (k1, k2)\n",
    "    accel_mag_train = np.sqrt(\n",
    "        df_train['Accel_X']**2 + df_train['Accel_Y']**2 + (df_train['Accel_Z'] - 9.8)**2\n",
    "    )\n",
    "    epsilon = 1e-6\n",
    "    k1 = (df_train['GPS_Speed'] / (accel_mag_train + epsilon)).mean()\n",
    "    k2 = (df_train['Engine_RPM'] / (accel_mag_train + epsilon)).mean()\n",
    "\n",
    "    # Train Isolation Forest Model\n",
    "    X_train_raw = df_train[sensor_cols].values\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "    iforest_model = IsolationForest(contamination=CONTAMINATION, random_state=RANDOM_STATE, n_estimators=100)\n",
    "    iforest_model.fit(X_train_scaled)\n",
    "\n",
    "    # Calculate Anomaly Threshold\n",
    "    train_scores = iforest_model.decision_function(X_train_scaled)\n",
    "    anomaly_threshold = np.percentile(train_scores, CONTAMINATION * 100)\n",
    "\n",
    "    print(\"âœ… Model Setup Complete.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 2. PREDICTION INTERFACE FUNCTION (CORE LOGIC)\n",
    "# =========================================================================\n",
    "\n",
    "def predict_fault_on_new_reading(accel_x, accel_y, accel_z, gps_speed, engine_rpm, row_number=\"N/A\"):\n",
    "    \"\"\"\n",
    "    Takes 5 sensor readings, runs both IF detection and physics-based attribution,\n",
    "    and prints a clear report, including the row number.\n",
    "    \"\"\"\n",
    "    global iforest_model, scaler, k1, k2, anomaly_threshold, sensor_cols\n",
    "    \n",
    "    # A. Format and Scale the raw input for IF model\n",
    "    raw_input_data = {\n",
    "        'Accel_X': accel_x, 'Accel_Y': accel_y, 'Accel_Z': accel_z, \n",
    "        'GPS_Speed': gps_speed, 'Engine_RPM': engine_rpm\n",
    "    }\n",
    "    df_new_input = pd.DataFrame([raw_input_data], columns=sensor_cols)\n",
    "    X_new_raw = df_new_input.values\n",
    "    X_new_scaled = scaler.transform(X_new_raw)\n",
    "\n",
    "    # B. Isolation Forest Detection\n",
    "    anomaly_score = iforest_model.decision_function(X_new_scaled)[0]\n",
    "    is_anomaly = (anomaly_score < anomaly_threshold)\n",
    "\n",
    "    # C. Physics-Based Correlation Check (Fault Attribution)\n",
    "    faulty_sensors = []\n",
    "    \n",
    "    accel_mag = np.sqrt(\n",
    "        accel_x**2 + accel_y**2 + (accel_z - 9.8)**2\n",
    "    )\n",
    "    expected_gps = k1 * accel_mag\n",
    "    expected_rpm = k2 * accel_mag\n",
    "    \n",
    "    # Check GPS_Speed deviation\n",
    "    if gps_speed > 1:\n",
    "        gps_dev_ratio = np.abs(gps_speed - expected_gps) / gps_speed\n",
    "        if gps_dev_ratio > DEVIATION_TOLERANCE:\n",
    "            faulty_sensors.append('GPS_Speed')\n",
    "        \n",
    "    # Check Engine_RPM deviation\n",
    "    if engine_rpm > 1:\n",
    "        rpm_dev_ratio = np.abs(engine_rpm - expected_rpm) / engine_rpm\n",
    "        if rpm_dev_ratio > DEVIATION_TOLERANCE:\n",
    "            faulty_sensors.append('Engine_RPM')\n",
    "        \n",
    "    # Final Attribution Logic\n",
    "    if is_anomaly and not faulty_sensors:\n",
    "        faulty_sensors.append('Accel_Sensor(s)')\n",
    "    elif not is_anomaly and faulty_sensors:\n",
    "        pass\n",
    "\n",
    "    # D. Print Report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    if is_anomaly or faulty_sensors:\n",
    "        print(f\"ðŸš¨ FAULT DETECTED in Row No: {row_number} ðŸš¨\")\n",
    "    else:\n",
    "        print(f\"ðŸŸ¢ NORMAL READING for Row No: {row_number} ðŸŸ¢\")\n",
    "        \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if faulty_sensors:\n",
    "        print(f\"Faulty Sensor(s) Identified: **{', '.join(faulty_sensors)}**\")\n",
    "        for sensor in faulty_sensors:\n",
    "            # Final required format: Sensor name and Row number\n",
    "            print(f\"-> **{sensor.upper()}** sensor has fault in it in row no **{row_number}**\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(f\"IF Anomaly Score: {anomaly_score:.4f} ({'ANOMALY' if is_anomaly else 'NORMAL'})\")\n",
    "        print(f\"Expected GPS: {expected_gps:.2f} (Actual: {gps_speed:.2f})\")\n",
    "        print(f\"Expected RPM: {expected_rpm:.2f} (Actual: {engine_rpm:.2f})\")\n",
    "    elif is_anomaly:\n",
    "        print(f\"IF Anomaly Score: {anomaly_score:.4f} (**ANOMALY**)\")\n",
    "        print(\"-> Suspect: Subtle correlation break, or Accel_Sensor(s) fault.\")\n",
    "    else:\n",
    "        print(f\"IF Anomaly Score: {anomaly_score:.4f} (NORMAL)\")\n",
    "        print(\"-> All sensor correlations are within normal limits.\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# =========================================================================\n",
    "# 3. TESTING LOOP FUNCTION\n",
    "# =========================================================================\n",
    "\n",
    "def test_model_on_test_data(file_name):\n",
    "    \"\"\"Loads the test data file and runs the prediction function on sample rows.\"\"\"\n",
    "    print(\"\\n\\n################ STARTING BATCH TEST EVALUATION ################\")\n",
    "    \n",
    "    try:\n",
    "        # Load the test data file (index_col=0 is used because CSVs often save an unnecessary index)\n",
    "        df_test = pd.read_csv(file_name, index_col=0) \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Test data file not found at '{file_name}'.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}. Check if column names in sensor_cols match the CSV headers.\")\n",
    "        return\n",
    "\n",
    "    if iforest_model is None or k1 is None:\n",
    "         print(\"âŒ Setup Error: Model or constants are not initialized. Run setup_model() first.\")\n",
    "         return\n",
    "\n",
    "    print(f\"Loaded {len(df_test)} records for testing.\")\n",
    "    \n",
    "    # Run the prediction function for a few sample rows (first 5)\n",
    "    test_indices = [0, 1, 2, 3, 4] \n",
    "    test_indices = sorted(list(set([i for i in test_indices if i < len(df_test)])))\n",
    "\n",
    "    for i in test_indices:\n",
    "        row = df_test.iloc[i]\n",
    "        \n",
    "        # Get the actual index/row name from the DataFrame to display in the output\n",
    "        actual_row_number = row.name \n",
    "        \n",
    "        # Call the interactive prediction function\n",
    "        predict_fault_on_new_reading(\n",
    "            accel_x=row['Accel_X'], \n",
    "            accel_y=row['Accel_Y'], \n",
    "            accel_z=row['Accel_Z'], \n",
    "            gps_speed=row['GPS_Speed'], \n",
    "            engine_rpm=row['RPM'],\n",
    "            row_number=actual_row_number # Pass the row number\n",
    "        )\n",
    "    \n",
    "    print(\"\\n################ BATCH TEST EVALUATION COMPLETE ################\")\n",
    "\n",
    "# =========================================================================\n",
    "# 4. EXECUTION\n",
    "# =========================================================================\n",
    "\n",
    "# Step 1: Initialize and train the model\n",
    "setup_model()\n",
    "\n",
    "# Step 2: Run the testing loop against the test file\n",
    "test_model_on_test_data(TEST_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dfaabcc-28f5-4906-8140-cd2f9accab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Fault Detection System (Training Model)...\n",
      "âœ… Model Setup Complete.\n",
      "\n",
      "--- Testing Custom Reading ---\n",
      "\n",
      "==================================================\n",
      "ðŸš¨ FAULT DETECTED in Row No: Custom_001 ðŸš¨\n",
      "==================================================\n",
      "Faulty Sensor(s) Identified: **Engine_RPM**\n",
      "-> **ENGINE_RPM** sensor has fault in it in row no **Custom_001**\n",
      "--------------------------------------------------\n",
      "IF Anomaly Score: 0.0421 (NORMAL)\n",
      "Expected GPS: 34.21 (Actual: 40.00)\n",
      "Expected RPM: 2837.20 (Actual: 26000.00)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. RUN SETUP (Assuming you have the setup_model() function defined) ---\n",
    "# This step must be executed once to initialize the model and constants.\n",
    "setup_model() \n",
    "\n",
    "# --- 2. DEFINE YOUR CUSTOM READING ---\n",
    "\n",
    "# Replace these values with your desired sensor readings:\n",
    "custom_accel_x = 0.5    \n",
    "custom_accel_y = 0.1\n",
    "custom_accel_z = 9.8 \n",
    "custom_gps_speed = 40.03\n",
    "custom_engine_rpm = 26000.0\n",
    "\n",
    "# --- 3. RUN THE PREDICTION FUNCTION ---\n",
    "\n",
    "print(\"\\n--- Testing Custom Reading ---\")\n",
    "\n",
    "# Call the function directly (row_number is set to \"Custom\" here)\n",
    "predict_fault_on_new_reading(\n",
    "    accel_x=custom_accel_x,\n",
    "    accel_y=custom_accel_y,\n",
    "    accel_z=custom_accel_z,\n",
    "    gps_speed=custom_gps_speed,\n",
    "    engine_rpm=custom_engine_rpm,\n",
    "    row_number=\"Custom_001\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1759ff-a3a8-4e4f-afe0-5f902e302219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
